# vi ~/.vimrc

#+BEGIN_SRC
set expandtab
set tabstop=2
set shiftwidth=2
set number
#+END_SRC

#+BEGIN_SRC
kubectl run -h
kubectl explain pods.spec.containers


tail -n500 -f /var/log/syslog | grep apiserver
tail -n500 /var/log/pods/***/kube-system_kub-apiserver*.log

k -n management get deploy
k -n management logs -h
k -n management logs deploy/collect-data -c nginx >> /root/logs.log

k -n management logs deploy/collect-data -c httpd >> /root/logs.log
#+END_SRC

#+BEGIN_SRC
kubectl create configmap myconfigmap --from-literal=key1=value1 --from-literal=key2=value2

kubectl create configmap trauerweide --from-literal=tree=trauerweide --dry-run=client  -o yaml > /root/cm.yaml


k -n world expose deploy europe --port 80
k -n world expose deploy asia --port 80
#+END_SRC


* Create a deployment to run nginx:alpine =>
#+BEGIN_SRC
kubectl create deployment nginx --image=nginx:alpine --dry-run=client -o=yaml --replicas=3 > my1.yaml
#+END_SRC

Create a deployment to run nginx:alpine =>
#+BEGIN_SRC
kubectl run pod1 --image=nginx --dry-run=client -o=yaml > my2.yaml
#+END_SRC

* Labels 

kubectl run command can use --Labels

<

#BEGIN_SRC
# Update deployment 'my-deployment' with the label 'unhealthy' and the value 'true'.
$ kubectl label deployment my-deployment unhealthy=true
  
# Update deployment 'my-deployment' with the label 'status' and the value 'unhealthy', overwriting any existing value.
$ kubectl label --overwrite deployment my-deployment status=unhealthy

#END_SRC


Create a deployment to run nginx:alpine  mount configmaps as environment variables to it =>

#+BEGIN_SRC

apiVersion: v1
kind: Pod
metadata:
  name: configmap-demo-pod
spec:
  containers:
    - name: demo
      image: alpine
      env:
        - name: NATURE
          valueFrom:
            configMapKeyRef:
              name: game-demo
              key: nature-key-name
      volumeMounts:
      - name: config1
        mountPath: "/config1"
        readOnly: true
      - name: config2
        mountPath: "/config2"
        readOnly: true
  volumes:
  - name: config1
    configMap:
      name: game-demo
  - name: config2
    configMap:
      name: game-demo
      items:
      - key: "game.properties"
        path: "game.properties"
      - key: "user-interface.properties"
        path: "user-interface.properties"

#+END_SRC

We need a new NetworkPolicy named np that restricts all Pods in Namespace space1 to only have outgoing traffic to Pods in Namespace space2 . Incoming traffic not affected.

We also need a new NetworkPolicy named np that restricts all Pods in Namespace space2 to only have incoming traffic from Pods in Namespace space1 . Outgoing traffic not affected.

The NetworkPolicies should still allow outgoing DNS traffic on port 53 TCP and UDP.

#+BEGIN_SRC
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np
  namespace: space1
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
     - namespaceSelector:
        matchLabels:
         kubernetes.io/metadata.name: space2
  - ports:
    - port: 53
      protocol: TCP
    - port: 53
      protocol: UDP

#+END_SRC

#+BEGIN_SRC
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: np
  namespace: space2
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
   - from:
     - namespaceSelector:
        matchLabels:
         kubernetes.io/metadata.name: space1
#+END_SRC
===
Clusterrole:

#+BEGIN_SRC

k get clusterrole view # there is default one

k create clusterrole -h # examples

k create rolebinding -h # examples

k auth can-i delete deployments --as system:serviceaccount:ns1:pipeline -n ns1
#+END_SRC

#+BEGIN_SRC
# create SAs
k -n ns1 create sa pipeline
k -n ns2 create sa pipeline

# use ClusterRole view
k get clusterrole view # there is default one
k create clusterrolebinding pipeline-view --clusterrole view --serviceaccount ns1:pipeline --serviceaccount ns2:pipeline

# manage Deployments in both Namespaces
k create clusterrole -h # examples
k create clusterrole pipeline-deployment-manager --verb create,delete --resource deployments
# instead of one ClusterRole we could also create the same Role in both Namespaces

k -n ns1 create rolebinding pipeline-deployment-manager --clusterrole pipeline-deployment-manager --serviceaccount ns1:pipeline
k -n ns2 create rolebinding pipeline-deployment-manager --clusterrole pipeline-deployment-manager --serviceaccount ns2:pipeline

#+END_SRC

#Check

#+BEGIN_SRC
# namespace ns1 deployment manager
k auth can-i delete deployments --as system:serviceaccount:ns1:pipeline -n ns1 # YES
k auth can-i create deployments --as system:serviceaccount:ns1:pipeline -n ns1 # YES
k auth can-i update deployments --as system:serviceaccount:ns1:pipeline -n ns1 # NO
k auth can-i update deployments --as system:serviceaccount:ns1:pipeline -n default # NO

# namespace ns2 deployment manager
k auth can-i delete deployments --as system:serviceaccount:ns2:pipeline -n ns2 # YES
k auth can-i create deployments --as system:serviceaccount:ns2:pipeline -n ns2 # YES
k auth can-i update deployments --as system:serviceaccount:ns2:pipeline -n ns2 # NO
k auth can-i update deployments --as system:serviceaccount:ns2:pipeline -n default # NO

# cluster wide view role
k auth can-i list deployments --as system:serviceaccount:ns1:pipeline -n ns1 # YES
k auth can-i list deployments --as system:serviceaccount:ns1:pipeline -A # YES
k auth can-i list pods --as system:serviceaccount:ns1:pipeline -A # YES
k auth can-i list pods --as system:serviceaccount:ns2:pipeline -A # YES
k auth can-i list secrets --as system:serviceaccount:ns2:pipeline -A # NO (default view-role doesn't allow)
#+END_SRC



* Daemonsets

#+BEGIN_SRC
kubectl get ds -n kube-system

#Daemonsets dont usually run on controller nodes. Toleration make daemonset run on control node also

kubectl create deploy mydaemon --image=nginx --dry-run=client > my.yaml

#then replace Deployment with Daemonset. And remove replicas and strategy

kubectl apply -f my.yaml

kubectl get pods -o wide

#+END_SRC


* Statefulset

#+BEGIN_SRC 
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx 
  serviceName: "nginx"
  replicas: 3 
  template:
    metadata:
      labels:
        app: nginx 
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteMany" ]
      resources:
        requests:
          storage: 1Gi
#+END_SRC


* scale

#+BEGIN_SRC 
kubectl scale deployment firstnginx --replicas 3
#+END_SRC

* k8s networking

[[./k8s_networking.jpg]]

#+BEGIN_SRC 

kubectl create deploy workshop --image=nginx --replicas=3

kubctl expose deploy workshop --type=NodePort --port=80

kubectl describe svc workshop

kubectl create service workshop-new --dry-run=client -o=yaml > mysvc.yaml
#+END_SRC

** ingress

#+BEGIN_SRC 
helm upgrade --index #get command from online

kubectl create ingress nginxsvc --class=nginx  --rule=mydomain.com/*=nginxsvc:80
kubectl port-forward -n ingress-nginx svc/ingress-nginx-controller 8080:80

kubectl describe ingress nginxsvc


#ingress with multiple rule#+BEGIN_SRC 
kubectl create ingress mygress --class=nginx  --rule=mydomain.com/*=nginxsvc:80 --rule=otherserver.org/*=otherserver:80
#+END_SRC


* maintenance

#+BEGIN_SRC 

kubectl get nodes
kubectl describe node worker1

systemctl start kubelet 

journalctl -u kubelet

#+END_SRC

* crictl

#+BEGIN_SRC 
sudo crictl ps

vi /etc/critcl.yaml


crictl ps

crictl pods

crictl inspect a12containerid34a

crictl image

crictl -h 
#+END_SRC

* staticpod

dont run it on controller node 

#+BEGIN_SRC 
kubectl run staticpod --image=nginx --dry-run=client -o=yaml > staticpod.yaml

cp staticpod.yaml  /etc/kubernetes/manifests/staticpod.yaml

#+END_SRC


* nodestate


kubectl cordon/uncordon

kubectl drain  , --ignore-daemonsets , --delete-emptydir-data

#+BEGIN_SRC

kubectl get nodes

kubectl cordon worker2

kubectl describe node worker2 | less

kubectl uncordon worker2

kubectl drain worker2 --ignore-daemonsets
#+END_SRC

* service account token

#+BEGIN_SRC
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
curl -H "Authorization: Bearer $TOKEN" --insecure https://kubernetes/api/v1/namespaces/default/pods
#+END_SRC